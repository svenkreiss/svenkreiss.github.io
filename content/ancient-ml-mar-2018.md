Title: Ancient ML
Date: 2018-03-01
Category: ML
Tags: ancient ML
Slug: ancient-ml-mar-2018
Summary: Ancient Machine Learning is a serious of paper reading notes. These are the notes for February 2018.
Status: draft


## A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, August 31, 1955 [@@mccarthy2006proposal], [PDF](https://www.aaai.org/ojs/index.php/aimagazine/article/download/1904/1802)

* The paper/event that gets credited with the foundation of the field of Artificial Intelligence research.
* The paper is three pages long and the authors include Claude Shannon.
* scale of the proposed project: 2 months, 10 men
* focused on language, abstraction and concepts
* Seven sections for improvement: Automatic Computers, How Can a Computer be
  Programmed to Use a Language, Neuron Nets, Theory of the Size of a Calculation,
  Self-Improvement, Abstractions, Randomness and Creativity
* Remarkable assumption in the proposal:

> the major obstacle is not lack of machine capacity, but our inability to write programs

* From the Wikipedia article on the [Dartmouth workshop](), it had the following attendees:
Dr. Marvin Minsky,
Dr. Julian Bigelow,
Professor D.M. Mackay,
Mr. Ray Solomonoff,
Mr. John Holland,
Mr. John McCarthy,
Dr. Claude Shannon,
Mr. Nathanial Rochester,
Mr. Oliver Selfridge,
Mr. Allen Newell,
Professor Herbert Simon.
* Full 102 pages of Ray Solomonoff's hand written notes are availble in
  [PDF format](http://raysolomonoff.com/dartmouth/notebook/notebook.html) including
  some doodles on page 3.


## The Mathematical Theory of Communication [@@shannon1951mathematical], [PDF](http://pubman.mpdl.mpg.de/pubman/item/escidoc:2383164/component/escidoc:2383163/Shannon_Weaver_1949_Mathematical.pdf)

* Central paper for many fields. 131 pages.
* Related: *The Idea Factory* is a book about Bell Labs around that time and makes it easier to visualize the people involved. Recommended read.

Starting with the paper by Weaver:

* Boltzmann observed in 1894 that entropy is related to "missing information" about the unobserved microstates
* components of a transmission: information source, transmitter, noise source, receiver, destination
* transmitter changes a _message_ into a _signal_, receiver does the reverse
* questions:
    * amount of information
    * capacity of a channel
    * efficient coding process
    * how does noise affect the _message_
    * effect of continuous signal rather than discrete symbols
* information is not attached to a particular message but to the amount of freedom of choice
* John Tukey suggested the word "bit" for "binary digit"
* example to visualize logarithmic nature of information: information contained in the state of a single relay is 1 and we want that the information contained in three relays to be 3, which is the case using the "logarithmic measure" because $log_2$ of $2^3$ is 3.
* think of a message as being generated by an ergodic Markoff process
* connection between information and entropy
* interesting simple example:
    * only two possible messages with $p_1$ and $p_2=1-p_1$
    * what is the amount of information as a function of $p_1$?
    * information is maximum for $p_1 = 0.5$, "when one is completely free to choose"
    * information goes to zero when one of the messages is very probable: "no freedom of choice -- no information"
* capacity of a communication channel
    * simple case: each symbol is made of $s$ bits and channel can transmit $n$ symbols per second, then the capacity is $ns$ bits per second
    * more interesting: varying lengths of symbols, only says it's measured in bits per second and does not reference the number of symbols
* fundamental theorem for a "noiseless channel transmitting discrete symbols":
    * channel with capacity $C$ bits per second
    * source of entropy/information with $H$ bits per second
    * statement: with proper coding it is possible to transmit symbols at a rate of up to $C/H$
* with noise, "the received signal exhibits greater information": signal is selected out of a larger set of possibilities; information is introduced from noise
* received _tentative message entropy_: with noise, one message symbol has high probability (and not $p=1$ anymore) and all other symbols acquire a small probability; ideally entropy is small
* _equivocation_: entropy of the message relative to the signal; weighted average of tentative message entropies over all possible signal symbols; "average uncertainty in the message when the signal is known"
* joint entropy (looking ahead to the definition given by Shannon): $$H(x,y) = H(x) + H(y|x) = H(y) + H(x|y)$$
    * $H(x)$ information of source signal
    * $H(y)$ information in received signal
    * $H(x|y)$ uncertainty in the sent signal if the received signal is known
    * $H(y|x)$ uncertainty in the received signal if the sent signal is known; "or the spurious part of the received signal information which is due to noise"
* useful transmitted information: $H(y)-H(y|x)$
* skipping continuous messages

## Backlog

* Online Convex Programming and Generalized Infinitesimal Gradient Ascent [@@zinkevich2003online], [PDF](http://www.aaai.org/Papers/ICML/2003/ICML03-120.pdf)
* Supervised Sequence Labelling with Recurrent Neural Networks [@@graves2012supervised], [PDF](https://www.cs.toronto.edu/~graves/preprint.pdf)

Discuss on Twitter and tweet at @svenkreiss with `#ancientml`.
